{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install CUDA C++ plugin for Colab:\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntOgKF_-y0SV",
        "outputId": "28bcf33e-61e6-47b1-8dc9-47b037a4c0ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpm54m5tr5\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect selected GPU and its NVIDA architecture:\n",
        "import subprocess\n",
        "gpu_info = subprocess.getoutput(\"nvidia-smi --query-gpu=name,compute_cap --format=csv,noheader,nounits\")\n",
        "if \"not found\" in gpu_info.lower(): raise RuntimeError(\"Error: No GPU found. Please select a GPU runtime environment.\")\n",
        "gpu_name, compute_cap = map(str.strip, gpu_info.split(','))\n",
        "gpu_arch = f\"sm_{compute_cap.replace('.', '')}\"\n",
        "\n",
        "print(f\"{'GPU Name':<15}: {gpu_name}\")\n",
        "print(f\"{'Architecture':<15}: {gpu_arch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVqh0baiO15d",
        "outputId": "3d1430fb-4ef4-4346-ace5-5260f70daf51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Name       : Tesla T4\n",
            "Architecture   : sm_75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void first_kernel(){\n",
        "    // blockDim(x,y,z) number of threads in the block\n",
        "    // blockIdx(x,y,z) index of block in the grid\n",
        "    // threadIdx(x,y,z) index of thread in the block\n",
        "    // globalId is the thread id globally\n",
        "    printf(\"blockDim = (%u, %u, %u)\\n\",blockDim.x, blockDim.y, blockDim.z);\n",
        "    int blockId = blockIdx.x;\n",
        "    int threadId = threadIdx.x;\n",
        "    int globalId = threadId + blockId * blockDim.x;\n",
        "    printf(\"Block id %d , Thread id in the block %d  Global thread id %d\\n\", blockId, threadId, globalId);\n",
        "}\n",
        "int main(){\n",
        "    int numBlocks=4;\n",
        "    int threadsPerBlock=8;\n",
        "    first_kernel<<<numBlocks, threadsPerBlock>>>();\n",
        "     cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "EKz7uxxzr8Oa",
        "outputId": "2ef955cf-fc0e-4e23-8491-a70b7b46d434",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "blockDim = (8, 1, 1)\n",
            "Block id 2 , Thread id in the block 0  Global thread id 16\n",
            "Block id 2 , Thread id in the block 1  Global thread id 17\n",
            "Block id 2 , Thread id in the block 2  Global thread id 18\n",
            "Block id 2 , Thread id in the block 3  Global thread id 19\n",
            "Block id 2 , Thread id in the block 4  Global thread id 20\n",
            "Block id 2 , Thread id in the block 5  Global thread id 21\n",
            "Block id 2 , Thread id in the block 6  Global thread id 22\n",
            "Block id 2 , Thread id in the block 7  Global thread id 23\n",
            "Block id 0 , Thread id in the block 0  Global thread id 0\n",
            "Block id 0 , Thread id in the block 1  Global thread id 1\n",
            "Block id 0 , Thread id in the block 2  Global thread id 2\n",
            "Block id 0 , Thread id in the block 3  Global thread id 3\n",
            "Block id 0 , Thread id in the block 4  Global thread id 4\n",
            "Block id 0 , Thread id in the block 5  Global thread id 5\n",
            "Block id 0 , Thread id in the block 6  Global thread id 6\n",
            "Block id 0 , Thread id in the block 7  Global thread id 7\n",
            "Block id 3 , Thread id in the block 0  Global thread id 24\n",
            "Block id 3 , Thread id in the block 1  Global thread id 25\n",
            "Block id 3 , Thread id in the block 2  Global thread id 26\n",
            "Block id 3 , Thread id in the block 3  Global thread id 27\n",
            "Block id 3 , Thread id in the block 4  Global thread id 28\n",
            "Block id 3 , Thread id in the block 5  Global thread id 29\n",
            "Block id 3 , Thread id in the block 6  Global thread id 30\n",
            "Block id 3 , Thread id in the block 7  Global thread id 31\n",
            "Block id 1 , Thread id in the block 0  Global thread id 8\n",
            "Block id 1 , Thread id in the block 1  Global thread id 9\n",
            "Block id 1 , Thread id in the block 2  Global thread id 10\n",
            "Block id 1 , Thread id in the block 3  Global thread id 11\n",
            "Block id 1 , Thread id in the block 4  Global thread id 12\n",
            "Block id 1 , Thread id in the block 5  Global thread id 13\n",
            "Block id 1 , Thread id in the block 6  Global thread id 14\n",
            "Block id 1 , Thread id in the block 7  Global thread id 15\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HW8N8RXzCTc",
        "outputId": "cd9a3f49-834c-4d39-8f83-45fa8457531f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Cell magic `%%cuda` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "daUodJGNggrR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}